#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆè¨ºæ–­ãƒ„ãƒ¼ãƒ« - åˆå¿ƒè€…å‘ã‘èª¬æ˜ä»˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³
SEOã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚’è¨ºæ–­
å„é …ç›®ã«ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜ã‚’è¿½åŠ 
"""

import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urlparse, urljoin
import ssl
import socket
from datetime import datetime
import json
import re
from collections import Counter

class WebsiteDiagnosisTool:
    def __init__(self, url):
        self.url = url
        self.parsed_url = urlparse(url)
        self.domain = self.parsed_url.netloc
        self.results = {
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'seo': {},
            'security': {},
            'performance': {},
            'accessibility': {},
            'overall_score': 0
        }
        
        # åˆå¿ƒè€…å‘ã‘èª¬æ˜
        self.explanations = self._get_explanations()
        
    def _get_explanations(self):
        """å„è¨ºæ–­é …ç›®ã®ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜"""
        return {
            'seo': {
                'title': {
                    'what': 'ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ã¯ã€ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¿ãƒ–ã‚„Googleæ¤œç´¢çµæœã«è¡¨ç¤ºã•ã‚Œã‚‹ã€Œãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã€ã§ã™ã€‚',
                    'why': 'ã‚ã‹ã‚Šã‚„ã™ã„ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹ã¨ã€æ¤œç´¢çµæœã§ã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã‚„ã™ããªã‚Šã¾ã™ã€‚',
                    'how': 'ã‚¿ã‚¤ãƒˆãƒ«ã¯30ã€œ60æ–‡å­—ãŒæœ€é©ã§ã™ã€‚ä¼šç¤¾åã‚„ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ç°¡æ½”ã«æ›¸ãã¾ã—ã‚‡ã†ã€‚'
                },
                'meta_description': {
                    'what': 'ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€Googleæ¤œç´¢çµæœã§ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸‹ã«è¡¨ç¤ºã•ã‚Œã‚‹ã€Œèª¬æ˜æ–‡ã€ã§ã™ã€‚',
                    'why': 'é­…åŠ›çš„ãªèª¬æ˜æ–‡ãŒã‚ã‚‹ã¨ã€æ¤œç´¢çµæœã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒå¢—ãˆã¾ã™ã€‚',
                    'how': '120ã€œ160æ–‡å­—ã§ã€ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¾ã—ã‚‡ã†ã€‚'
                },
                'h1': {
                    'what': 'H1ã‚¿ã‚°ã¯ã€ãƒšãƒ¼ã‚¸ã®ã€Œå¤§è¦‹å‡ºã—ã€ã§ã™ã€‚æ–°èã®1é¢ã®å¤§ããªè¦‹å‡ºã—ã®ã‚ˆã†ãªã‚‚ã®ã§ã™ã€‚',
                    'why': 'GoogleãŒãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®é‡è¦ãªæ‰‹ãŒã‹ã‚Šã«ãªã‚Šã¾ã™ã€‚',
                    'how': 'H1ã‚¿ã‚°ã¯1ãƒšãƒ¼ã‚¸ã«1ã¤ã ã‘é…ç½®ã—ã€ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’è¡¨ã™è¦‹å‡ºã—ã«ã—ã¾ã—ã‚‡ã†ã€‚'
                },
                'alt': {
                    'what': 'altå±æ€§ã¯ã€ç”»åƒã®ã€Œèª¬æ˜æ–‡ã€ã§ã™ã€‚ç”»åƒãŒè¡¨ç¤ºã•ã‚Œãªã„æ™‚ã‚„ã€ç›®ã®ä¸è‡ªç”±ãªæ–¹ã®ãŸã‚ã«ä½¿ã‚ã‚Œã¾ã™ã€‚',
                    'why': 'Googleã¯ç”»åƒã®å†…å®¹ã‚’ç†è§£ã§ããªã„ã®ã§ã€altå±æ€§ã§èª¬æ˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚',
                    'how': 'å„ç”»åƒã«ã€Œä½•ã®ç”»åƒã‹ã€ã‚’ç°¡æ½”ã«èª¬æ˜ã™ã‚‹æ–‡ç« ã‚’ä»˜ã‘ã¾ã—ã‚‡ã†ã€‚'
                }
            },
            'security': {
                'https': {
                    'what': 'HTTPSã¯ã€ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã¨ã®é€šä¿¡ã‚’ã€Œæš—å·åŒ–ã€ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å—äº¬éŒ ã®ãƒãƒ¼ã‚¯ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚',
                    'why': 'HTTPSãŒãªã„ã¨ã€å…¥åŠ›ã—ãŸæƒ…å ±ï¼ˆãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ç•ªå·ãªã©ï¼‰ãŒç›—ã¾ã‚Œã‚‹å±é™ºãŒã‚ã‚Šã¾ã™ã€‚',
                    'how': 'ã‚µãƒ¼ãƒãƒ¼ä¼šç¤¾ã«ã€ŒSSLè¨¼æ˜æ›¸ã€ã‚’ç”³è«‹ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆå¤šãã¯ç„¡æ–™ï¼‰ã€‚',
                    'risk': 'ã€å±é™ºåº¦ï¼šé«˜ã€‘HTTPSãŒãªã„ã‚µã‚¤ãƒˆã¯ã€GoogleãŒã€Œå®‰å…¨ã§ãªã„ã€ã¨è­¦å‘Šã‚’è¡¨ç¤ºã—ã¾ã™ã€‚'
                },
                'strict_transport_security': {
                    'what': 'HSTSï¼ˆHTTP Strict Transport Securityï¼‰ã¯ã€ã€Œå¿…ãšHTTPSã§æ¥ç¶šã™ã‚‹ã€ã¨ã„ã†æŒ‡ç¤ºã§ã™ã€‚',
                    'why': 'æ‚ªæ„ã®ã‚ã‚‹äººãŒã€HTTPSã‚’HTTPã«å¤‰ãˆã¦æƒ…å ±ã‚’ç›—ã‚€æ”»æ’ƒã‚’é˜²ãã¾ã™ã€‚',
                    'how': 'ã‚µãƒ¼ãƒãƒ¼ã®è¨­å®šã§ã€ŒStrict-Transport-Securityã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™ã€‚',
                    'risk': 'ã€å±é™ºåº¦ï¼šä¸­ã€‘HTTPSã‚’ä½¿ã£ã¦ã„ã¦ã‚‚ã€ã“ã®è¨­å®šãŒãªã„ã¨ä¸€éƒ¨ã®æ”»æ’ƒã‚’é˜²ã’ã¾ã›ã‚“ã€‚'
                },
                'x_frame_options': {
                    'what': 'X-Frame-Optionsã¯ã€ã€Œä»–ã®ã‚µã‚¤ãƒˆã«åŸ‹ã‚è¾¼ã¾ã‚Œã‚‹ã®ã‚’é˜²ãã€è¨­å®šã§ã™ã€‚',
                    'why': 'æ‚ªæ„ã®ã‚ã‚‹ã‚µã‚¤ãƒˆãŒã‚ãªãŸã®ã‚µã‚¤ãƒˆã‚’åŸ‹ã‚è¾¼ã‚“ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é¨™ã™æ”»æ’ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã‚¸ãƒ£ãƒƒã‚­ãƒ³ã‚°ï¼‰ã‚’é˜²ãã¾ã™ã€‚',
                    'how': 'ã‚µãƒ¼ãƒãƒ¼ã®è¨­å®šã§ã€ŒX-Frame-Options: DENYã€ã¾ãŸã¯ã€ŒSAMEORIGINã€ã‚’è¿½åŠ ã—ã¾ã™ã€‚',
                    'risk': 'ã€å±é™ºåº¦ï¼šä¸­ã€‘ã“ã®è¨­å®šãŒãªã„ã¨ã€å½ã®ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ãªã©ã«æ‚ªç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'
                },
                'content_security_policy': {
                    'what': 'CSPï¼ˆContent Security Policyï¼‰ã¯ã€ã€Œã©ã“ã‹ã‚‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€ã‹ã€ã‚’åˆ¶é™ã™ã‚‹è¨­å®šã§ã™ã€‚',
                    'why': 'æ‚ªæ„ã®ã‚ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå‹æ‰‹ã«å®Ÿè¡Œã•ã‚Œã‚‹ã®ã‚’é˜²ãã¾ã™ï¼ˆXSSæ”»æ’ƒå¯¾ç­–ï¼‰ã€‚',
                    'how': 'ã‚µãƒ¼ãƒãƒ¼ã®è¨­å®šã§ã€ŒContent-Security-Policyã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™ã€‚',
                    'risk': 'ã€å±é™ºåº¦ï¼šä¸­ã€œé«˜ã€‘ã“ã®è¨­å®šãŒãªã„ã¨ã€ã‚µã‚¤ãƒˆã«ä¸æ­£ãªã‚³ãƒ¼ãƒ‰ã‚’åŸ‹ã‚è¾¼ã¾ã‚Œã‚‹å±é™ºãŒã‚ã‚Šã¾ã™ã€‚'
                }
            },
            'performance': {
                'load_time': {
                    'what': 'ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿æ™‚é–“ã¯ã€ã‚µã‚¤ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ã«ã‹ã‹ã‚‹æ™‚é–“ã§ã™ã€‚',
                    'why': 'èª­ã¿è¾¼ã¿ãŒé…ã„ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¾…ã¡ãã‚Œãšã«é›¢è„±ã—ã¦ã—ã¾ã„ã¾ã™ï¼ˆ3ç§’ä»¥ä¸Šã§åŠæ•°ãŒé›¢è„±ï¼‰ã€‚',
                    'how': 'ç”»åƒã‚’åœ§ç¸®ã™ã‚‹ã€ä¸è¦ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å‰Šé™¤ã™ã‚‹ã€ã‚µãƒ¼ãƒãƒ¼ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãªã©ã®æ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚'
                },
                'page_size': {
                    'what': 'ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã¯ã€ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆMBï¼‰ã§ã™ã€‚',
                    'why': 'ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºãŒå¤§ãã„ã¨ã€èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚Šã€ã‚¹ãƒãƒ›ã®ãƒ‡ãƒ¼ã‚¿é€šä¿¡é‡ã‚‚å¢—ãˆã¾ã™ã€‚',
                    'how': 'ç”»åƒã‚’åœ§ç¸®ã™ã‚‹ã€ä¸è¦ãªã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã™ã‚‹ã€å‹•ç”»ã¯å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ã†ãªã©ã€‚'
                },
                'compression': {
                    'what': 'åœ§ç¸®ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œzipå½¢å¼ã€ã®ã‚ˆã†ã«å°ã•ãã—ã¦é€ã‚‹æŠ€è¡“ã§ã™ã€‚',
                    'why': 'åœ§ç¸®ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿é‡ãŒ50ã€œ70%æ¸›å°‘ã—ã€èª­ã¿è¾¼ã¿ãŒé€Ÿããªã‚Šã¾ã™ã€‚',
                    'how': 'ã‚µãƒ¼ãƒãƒ¼ã®è¨­å®šã§ã€ŒGzipåœ§ç¸®ã€ã¾ãŸã¯ã€ŒBrotliåœ§ç¸®ã€ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚'
                }
            },
            'accessibility': {
                'lang': {
                    'what': 'langå±æ€§ã¯ã€ã€Œã“ã®ãƒšãƒ¼ã‚¸ã¯ä½•èªã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã‹ã€ã‚’ç¤ºã™ã‚‚ã®ã§ã™ã€‚',
                    'why': 'ç›®ã®ä¸è‡ªç”±ãªæ–¹ãŒä½¿ã†ã€Œèª­ã¿ä¸Šã’ã‚½ãƒ•ãƒˆã€ãŒã€æ­£ã—ã„ç™ºéŸ³ã§èª­ã¿ä¸Šã’ã‚‹ãŸã‚ã«å¿…è¦ã§ã™ã€‚',
                    'how': 'HTMLã®æœ€åˆã« <html lang="ja"> ã®ã‚ˆã†ã«è¨€èªã‚’æŒ‡å®šã—ã¾ã™ï¼ˆæ—¥æœ¬èªã¯"ja"ï¼‰ã€‚'
                },
                'main_landmark': {
                    'what': 'mainãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã¯ã€ã€Œãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã“ã“ã€ã¨ã„ã†ç›®å°ã§ã™ã€‚',
                    'why': 'ç›®ã®ä¸è‡ªç”±ãªæ–¹ãŒã€èª­ã¿ä¸Šã’ã‚½ãƒ•ãƒˆã§ã€Œæœ¬æ–‡ã«ã‚¸ãƒ£ãƒ³ãƒ—ã€ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚',
                    'how': 'ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ <main> ã‚¿ã‚°ã§å›²ã¿ã¾ã™ã€‚'
                }
            }
        }
    
    def diagnose(self):
        """å…¨ã¦ã®è¨ºæ–­ã‚’å®Ÿè¡Œï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
        return self.run_diagnosis()
    
    def run_diagnosis(self):
        """å…¨ã¦ã®è¨ºæ–­ã‚’å®Ÿè¡Œï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰"""
        print(f"ğŸ” è¨ºæ–­é–‹å§‹: {self.url}\n")
        
        # ãƒšãƒ¼ã‚¸ã®å–å¾—
        try:
            start_time = time.time()
            response = requests.get(self.url, timeout=30, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            load_time = time.time() - start_time
            
            self.response = response
            self.soup = BeautifulSoup(response.content, 'html.parser')
            self.load_time = load_time
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ - {str(e)}")
            return None
        
        # å„è¨ºæ–­ã®å®Ÿè¡Œ
        self.diagnose_seo()
        self.diagnose_security()
        self.diagnose_performance()
        self.diagnose_accessibility()
        
        # ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        self.calculate_overall_score()
        
        return self.results
    
    def diagnose_seo(self):
        """SEOè¨ºæ–­"""
        print("ğŸ“Š SEOè¨ºæ–­ä¸­...")
        seo = {}
        issues = []
        success = []
        explanations = []
        score = 0
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°
        title = self.soup.find('title')
        if title and title.string:
            title_text = title.string.strip()
            seo['title'] = title_text
            seo['title_length'] = len(title_text)
            if 30 <= len(title_text) <= 60:
                score += 15
                success.append(f"Title tag is configured ({len(title_text)} chars)")
            else:
                issues.append(f"Title length is not optimal (current: {len(title_text)} chars, recommended: 30-60 chars)")
                explanations.append({
                    'issue': 'Title length',
                    'explanation': self.explanations['seo']['title']
                })
        else:
            issues.append("Title tag not found")
            explanations.append({
                'issue': 'Title tag missing',
                'explanation': self.explanations['seo']['title']
            })
            seo['title'] = None
        
        # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³
        meta_desc = self.soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            desc_text = meta_desc.get('content').strip()
            seo['meta_description'] = desc_text
            seo['meta_description_length'] = len(desc_text)
            if 120 <= len(desc_text) <= 160:
                score += 15
                success.append(f"Meta description is configured ({len(desc_text)} chars)")
            else:
                issues.append(f"Meta description length is not optimal (current: {len(desc_text)} chars, recommended: 120-160 chars)")
                explanations.append({
                    'issue': 'Meta description length',
                    'explanation': self.explanations['seo']['meta_description']
                })
        else:
            issues.append("Meta description not found")
            explanations.append({
                'issue': 'Meta description missing',
                'explanation': self.explanations['seo']['meta_description']
            })
            seo['meta_description'] = None
        
        # è¦‹å‡ºã—ã‚¿ã‚°ã®åˆ†æ
        headings = {f'h{i}': [] for i in range(1, 7)}
        for i in range(1, 7):
            tags = self.soup.find_all(f'h{i}')
            headings[f'h{i}'] = [tag.get_text().strip() for tag in tags]
        
        seo['headings'] = headings
        
        # H1ã‚¿ã‚°ã®ãƒã‚§ãƒƒã‚¯
        h1_count = len(headings['h1'])
        if h1_count == 1:
            score += 10
            success.append("Single H1 tag found")
        elif h1_count == 0:
            issues.append("H1 tag not found")
            explanations.append({
                'issue': 'H1 tag missing',
                'explanation': self.explanations['seo']['h1']
            })
        else:
            issues.append(f"Multiple H1 tags found ({h1_count})")
            explanations.append({
                'issue': 'Multiple H1 tags',
                'explanation': self.explanations['seo']['h1']
            })
        
        # è¦‹å‡ºã—æ§‹é€ ã®ãƒã‚§ãƒƒã‚¯
        if headings['h2']:
            score += 5
        else:
            issues.append("H2 tag not found")
        
        # ç”»åƒã®altå±æ€§ãƒã‚§ãƒƒã‚¯
        images = self.soup.find_all('img')
        total_images = len(images)
        images_with_alt = sum(1 for img in images if img.get('alt'))
        
        seo['total_images'] = total_images
        seo['images_with_alt'] = images_with_alt
        
        if total_images > 0:
            alt_ratio = images_with_alt / total_images
            if alt_ratio >= 0.9:
                score += 15
                success.append(f"Most images have alt attributes ({images_with_alt}/{total_images})")
            elif alt_ratio >= 0.7:
                score += 10
                issues.append(f"Some images missing alt attributes ({images_with_alt}/{total_images})")
                explanations.append({
                    'issue': 'Missing alt attributes',
                    'explanation': self.explanations['seo']['alt']
                })
            else:
                issues.append(f"Many images missing alt attributes ({images_with_alt}/{total_images})")
                explanations.append({
                    'issue': 'Missing alt attributes',
                    'explanation': self.explanations['seo']['alt']
                })
        
        # Open Graphã‚¿ã‚°
        og_tags = {}
        for og in self.soup.find_all('meta', property=re.compile(r'^og:')):
            og_tags[og.get('property')] = og.get('content')
        seo['open_graph'] = og_tags
        if og_tags:
            score += 10
            success.append("Open Graph tags configured")
        
        # Twitter Cardã‚¿ã‚°
        twitter_tags = {}
        for twitter in self.soup.find_all('meta', attrs={'name': re.compile(r'^twitter:')}):
            twitter_tags[twitter.get('name')] = twitter.get('content')
        seo['twitter_card'] = twitter_tags
        if twitter_tags:
            score += 5
            success.append("Twitter Card tags configured")
        
        # Canonicalã‚¿ã‚°
        canonical = self.soup.find('link', rel='canonical')
        seo['canonical'] = canonical.get('href') if canonical else None
        if canonical:
            score += 5
            success.append("Canonical tag configured")
        
        # å†…éƒ¨ãƒªãƒ³ã‚¯æ•°
        internal_links = []
        external_links = []
        for link in self.soup.find_all('a', href=True):
            href = link.get('href')
            if href.startswith('http'):
                if self.domain in href:
                    internal_links.append(href)
                else:
                    external_links.append(href)
            elif href.startswith('/'):
                internal_links.append(href)
        
        seo['internal_links_count'] = len(internal_links)
        seo['external_links_count'] = len(external_links)
        
        if len(internal_links) > 0:
            score += 5
            success.append(f"Internal links found ({len(internal_links)})")
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
        structured_data = []
        for script in self.soup.find_all('script', type='application/ld+json'):
            try:
                data = json.loads(script.string)
                structured_data.append(data)
            except:
                pass
        seo['structured_data'] = structured_data
        if structured_data:
            score += 10
            success.append("Structured data found")
        
        seo['score'] = min(score, 100)
        seo['issues'] = issues
        seo['success'] = success
        seo['explanations'] = explanations
        self.results['seo'] = seo
        print(f"  âœ… SEOã‚¹ã‚³ã‚¢: {seo['score']}/100")
    
    def diagnose_security(self):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨ºæ–­"""
        print("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨ºæ–­ä¸­...")
        security = {}
        issues = []
        success = []
        explanations = []
        score = 0
        
        # HTTPSä½¿ç”¨ç¢ºèª
        is_https = self.parsed_url.scheme == 'https'
        security['https'] = is_https
        if is_https:
            score += 30
            success.append("HTTPS enabled")
        else:
            issues.append("HTTPS not enabled (security risk)")
            explanations.append({
                'issue': 'HTTPS not enabled',
                'explanation': self.explanations['security']['https']
            })
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
        headers = self.response.headers
        security_headers = {
            'Strict-Transport-Security': headers.get('Strict-Transport-Security'),
            'X-Frame-Options': headers.get('X-Frame-Options'),
            'X-Content-Type-Options': headers.get('X-Content-Type-Options'),
            'X-XSS-Protection': headers.get('X-XSS-Protection'),
            'Content-Security-Policy': headers.get('Content-Security-Policy'),
            'Referrer-Policy': headers.get('Referrer-Policy'),
            'Permissions-Policy': headers.get('Permissions-Policy')
        }
        
        security['security_headers'] = security_headers
        
        # å„ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        header_scores = {
            'Strict-Transport-Security': 15,
            'X-Frame-Options': 10,
            'X-Content-Type-Options': 10,
            'X-XSS-Protection': 5,
            'Content-Security-Policy': 20,
            'Referrer-Policy': 5,
            'Permissions-Policy': 5
        }
        
        header_explanations = {
            'Strict-Transport-Security': self.explanations['security']['strict_transport_security'],
            'X-Frame-Options': self.explanations['security']['x_frame_options'],
            'Content-Security-Policy': self.explanations['security']['content_security_policy']
        }
        
        for header, value in security_headers.items():
            if value:
                score += header_scores.get(header, 0)
                success.append(f"{header} header configured")
            else:
                issues.append(f"{header} header not set")
                if header in header_explanations:
                    explanations.append({
                        'issue': f'{header} missing',
                        'explanation': header_explanations[header]
                    })
        
        # SSLè¨¼æ˜æ›¸ã®ç¢ºèªï¼ˆHTTPSã®å ´åˆï¼‰
        if is_https:
            try:
                context = ssl.create_default_context()
                with socket.create_connection((self.domain, 443), timeout=10) as sock:
                    with context.wrap_socket(sock, server_hostname=self.domain) as ssock:
                        cert = ssock.getpeercert()
                        security['ssl_certificate'] = {
                            'issued_to': cert.get('subject', []),
                            'issued_by': cert.get('issuer', []),
                            'valid_from': cert.get('notBefore'),
                            'valid_until': cert.get('notAfter')
                        }
                        success.append("Valid SSL certificate")
            except Exception as e:
                issues.append(f"SSL certificate check failed: {str(e)}")
                security['ssl_certificate'] = None
        
        security['score'] = min(score, 100)
        security['issues'] = issues
        security['success'] = success
        security['explanations'] = explanations
        self.results['security'] = security
        print(f"  âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {security['score']}/100")
    
    def diagnose_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ºæ–­"""
        print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ºæ–­ä¸­...")
        performance = {}
        issues = []
        success = []
        explanations = []
        score = 0
        
        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿æ™‚é–“
        performance['load_time'] = round(self.load_time, 3)
        if self.load_time < 1:
            score += 30
            success.append(f"Fast page load time ({round(self.load_time, 2)}s)")
        elif self.load_time < 2:
            score += 20
            issues.append(f"Page load time is slightly slow ({round(self.load_time, 2)}s)")
            explanations.append({
                'issue': 'Slow load time',
                'explanation': self.explanations['performance']['load_time']
            })
        elif self.load_time < 3:
            score += 10
            issues.append(f"Page load time is slow ({round(self.load_time, 2)}s)")
            explanations.append({
                'issue': 'Slow load time',
                'explanation': self.explanations['performance']['load_time']
            })
        else:
            issues.append(f"Page load time is very slow ({round(self.load_time, 2)}s)")
            explanations.append({
                'issue': 'Very slow load time',
                'explanation': self.explanations['performance']['load_time']
            })
        
        # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º
        page_size = len(self.response.content)
        performance['page_size_bytes'] = page_size
        performance['page_size_kb'] = round(page_size / 1024, 2)
        
        if page_size < 500 * 1024:
            score += 20
            success.append(f"Appropriate page size ({round(page_size/1024, 2)}KB)")
        elif page_size < 1024 * 1024:
            score += 15
            issues.append(f"Page size is slightly large ({round(page_size/1024, 2)}KB)")
            explanations.append({
                'issue': 'Large page size',
                'explanation': self.explanations['performance']['page_size']
            })
        elif page_size < 3 * 1024 * 1024:
            score += 5
            issues.append(f"Page size is large ({round(page_size/1024/1024, 2)}MB)")
            explanations.append({
                'issue': 'Large page size',
                'explanation': self.explanations['performance']['page_size']
            })
        else:
            issues.append(f"Page size is very large ({round(page_size/1024/1024, 2)}MB)")
            explanations.append({
                'issue': 'Very large page size',
                'explanation': self.explanations['performance']['page_size']
            })
        
        # ãƒªã‚½ãƒ¼ã‚¹æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ
        resources = {
            'scripts': len(self.soup.find_all('script')),
            'stylesheets': len(self.soup.find_all('link', rel='stylesheet')),
            'images': len(self.soup.find_all('img')),
            'iframes': len(self.soup.find_all('iframe'))
        }
        performance['resources'] = resources
        
        total_resources = sum(resources.values())
        if total_resources < 30:
            score += 15
            success.append(f"Appropriate number of resources ({total_resources})")
        elif total_resources < 50:
            score += 10
            success.append(f"Moderate number of resources ({total_resources})")
        else:
            issues.append(f"Too many resources (total: {total_resources})")
        
        # åœ§ç¸®ã®ç¢ºèª
        content_encoding = self.response.headers.get('Content-Encoding')
        performance['compression'] = content_encoding
        if content_encoding in ['gzip', 'br', 'deflate']:
            score += 15
            success.append(f"Content compression enabled ({content_encoding})")
        else:
            issues.append("Content compression not enabled")
            explanations.append({
                'issue': 'No compression',
                'explanation': self.explanations['performance']['compression']
            })
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¶å¾¡
        cache_control = self.response.headers.get('Cache-Control')
        performance['cache_control'] = cache_control
        if cache_control:
            score += 10
            success.append("Cache-Control header configured")
        else:
            issues.append("Cache-Control header not set")
        
        performance['score'] = min(score, 100)
        performance['issues'] = issues
        performance['success'] = success
        performance['explanations'] = explanations
        self.results['performance'] = performance
        print(f"  âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {performance['score']}/100")
    
    def diagnose_accessibility(self):
        """ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£è¨ºæ–­"""
        print("â™¿ ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£è¨ºæ–­ä¸­...")
        accessibility = {}
        issues = []
        success = []
        explanations = []
        score = 0
        
        # è¨€èªå±æ€§
        html_tag = self.soup.find('html')
        lang = html_tag.get('lang') if html_tag else None
        accessibility['lang_attribute'] = lang
        if lang:
            score += 15
            success.append(f"HTML lang attribute present ({lang})")
        else:
            issues.append("HTML element missing lang attribute")
            explanations.append({
                'issue': 'Missing lang attribute',
                'explanation': self.explanations['accessibility']['lang']
            })
        
        # ç”»åƒã®altå±æ€§
        images = self.soup.find_all('img')
        images_without_alt = [img for img in images if not img.get('alt')]
        accessibility['images_without_alt_count'] = len(images_without_alt)
        
        if len(images) > 0:
            alt_ratio = (len(images) - len(images_without_alt)) / len(images)
            if alt_ratio == 1:
                score += 20
                success.append("All images have alt attributes")
            elif alt_ratio >= 0.8:
                score += 15
                issues.append(f"{len(images_without_alt)} images missing alt attributes")
            else:
                issues.append(f"Many images missing alt attributes ({len(images_without_alt)}/{len(images)})")
        
        # ãƒ•ã‚©ãƒ¼ãƒ ãƒ©ãƒ™ãƒ«
        inputs = self.soup.find_all(['input', 'textarea', 'select'])
        inputs_with_labels = 0
        for inp in inputs:
            inp_id = inp.get('id')
            aria_label = inp.get('aria-label')
            if inp_id and self.soup.find('label', attrs={'for': inp_id}):
                inputs_with_labels += 1
            elif aria_label:
                inputs_with_labels += 1
        
        accessibility['form_inputs_count'] = len(inputs)
        accessibility['inputs_with_labels'] = inputs_with_labels
        
        if len(inputs) > 0:
            if inputs_with_labels == len(inputs):
                score += 15
                success.append("All form elements have labels")
            elif inputs_with_labels >= len(inputs) * 0.7:
                score += 10
                issues.append(f"Some form elements missing labels ({inputs_with_labels}/{len(inputs)})")
            else:
                issues.append(f"Many form elements missing labels ({inputs_with_labels}/{len(inputs)})")
        
        # ARIAå±æ€§ã®ä½¿ç”¨
        aria_elements = self.soup.find_all(attrs={'role': True})
        aria_labels = self.soup.find_all(attrs={'aria-label': True})
        accessibility['aria_roles_count'] = len(aria_elements)
        accessibility['aria_labels_count'] = len(aria_labels)
        
        if len(aria_elements) > 0 or len(aria_labels) > 0:
            score += 10
            success.append(f"ARIA attributes used ({len(aria_elements)} roles, {len(aria_labels)} labels)")
        
        # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯è¦ç´ 
        landmarks = ['header', 'nav', 'main', 'footer', 'aside', 'section', 'article']
        found_landmarks = {tag: len(self.soup.find_all(tag)) for tag in landmarks}
        accessibility['landmarks'] = found_landmarks
        
        if found_landmarks['main'] > 0:
            score += 10
            success.append("Main landmark found")
        else:
            issues.append("Main landmark not found")
            explanations.append({
                'issue': 'Missing main landmark',
                'explanation': self.explanations['accessibility']['main_landmark']
            })
        
        if found_landmarks['nav'] > 0:
            score += 5
            success.append("Navigation landmark found")
        
        # è¦‹å‡ºã—ã®éšå±¤æ§‹é€ 
        headings_order = []
        for i in range(1, 7):
            for tag in self.soup.find_all(f'h{i}'):
                headings_order.append(i)
        
        heading_skip = False
        for i in range(len(headings_order) - 1):
            if headings_order[i+1] - headings_order[i] > 1:
                heading_skip = True
                break
        
        if not heading_skip and len(headings_order) > 0:
            score += 10
            success.append("Proper heading hierarchy")
        elif heading_skip:
            issues.append("Heading hierarchy issues (e.g., h4 after h2)")
        
        # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚§ãƒƒã‚¯
        links = self.soup.find_all('a')
        empty_links = [link for link in links if not link.get_text().strip() and not link.get('aria-label')]
        accessibility['empty_links_count'] = len(empty_links)
        
        if len(empty_links) == 0 and len(links) > 0:
            score += 10
            success.append("All links have text")
        elif len(empty_links) > 0:
            issues.append(f"{len(empty_links)} links without text")
        
        accessibility['score'] = min(score, 100)
        accessibility['issues'] = issues
        accessibility['success'] = success
        accessibility['explanations'] = explanations
        self.results['accessibility'] = accessibility
        print(f"  âœ… ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {accessibility['score']}/100")
    
    def calculate_overall_score(self):
        """ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        weights = {
            'seo': 0.3,
            'security': 0.3,
            'performance': 0.2,
            'accessibility': 0.2
        }
        
        overall = 0
        for category, weight in weights.items():
            overall += self.results[category]['score'] * weight
        
        self.results['overall_score'] = round(overall, 1)
        
        # Streamlitã‚¢ãƒ—ãƒªç”¨ã« scores ã‚­ãƒ¼ã‚’è¿½åŠ 
        self.results['scores'] = {
            'seo': self.results['seo']['score'],
            'security': self.results['security']['score'],
            'performance': self.results['performance']['score'],
            'accessibility': self.results['accessibility']['score']
        }
        
        print(f"\nğŸ¯ ç·åˆã‚¹ã‚³ã‚¢: {self.results['overall_score']}/100")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 80)
    print("ğŸ” ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆè¨ºæ–­ãƒ„ãƒ¼ãƒ«ï¼ˆèª¬æ˜ä»˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰")
    print("=" * 80)
    print("")
    
    url = input("è¨ºæ–­ã™ã‚‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    
    if not url:
        print("âŒ URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    print("")
    
    tool = WebsiteDiagnosisTool(url)
    results = tool.run_diagnosis()
    
    if results:
        # èª¬æ˜ä»˜ããƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
        print("\n" + "=" * 80)
        print("ğŸ“‹ è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆï¼ˆè©³ç´°èª¬æ˜ä»˜ãï¼‰")
        print("=" * 80)
        
        for category in ['seo', 'security', 'performance', 'accessibility']:
            data = results[category]
            if data.get('explanations'):
                print(f"\nã€{category.upper()}ã€‘")
                for exp in data['explanations']:
                    print(f"\nâš ï¸ {exp['issue']}")
                    print(f"  ğŸ“ {exp['explanation']['what']}")
                    print(f"  ğŸ’¡ {exp['explanation']['why']}")
                    print(f"  ğŸ”§ {exp['explanation']['how']}")
                    if 'risk' in exp['explanation']:
                        print(f"  âš ï¸ {exp['explanation']['risk']}")
        
        # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = f"diagnosis_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\n\nğŸ’¾ è©³ç´°ãªè¨ºæ–­çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
