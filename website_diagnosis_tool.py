#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆè¨ºæ–­ãƒ„ãƒ¼ãƒ« - åŒ…æ‹¬çš„ãªè¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ 
SEOã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚’è¨ºæ–­
"""

import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urlparse, urljoin
import ssl
import socket
from datetime import datetime
import json
import re
from collections import Counter

class WebsiteDiagnosisTool:
    def __init__(self, url):
        self.url = url
        self.parsed_url = urlparse(url)
        self.domain = self.parsed_url.netloc
        self.results = {
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'seo': {},
            'security': {},
            'performance': {},
            'accessibility': {},
            'overall_score': 0
        }
        
    def run_diagnosis(self):
        """å…¨ã¦ã®è¨ºæ–­ã‚’å®Ÿè¡Œ"""
        print(f"ğŸ” è¨ºæ–­é–‹å§‹: {self.url}\n")
        
        # ãƒšãƒ¼ã‚¸ã®å–å¾—
        try:
            start_time = time.time()
            response = requests.get(self.url, timeout=30, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            load_time = time.time() - start_time
            
            self.response = response
            self.soup = BeautifulSoup(response.content, 'html.parser')
            self.load_time = load_time
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ - {str(e)}")
            return None
        
        # å„è¨ºæ–­ã®å®Ÿè¡Œ
        self.diagnose_seo()
        self.diagnose_security()
        self.diagnose_performance()
        self.diagnose_accessibility()
        
        # ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        self.calculate_overall_score()
        
        return self.results
    
    def diagnose_seo(self):
        """SEOè¨ºæ–­"""
        print("ğŸ“Š SEOè¨ºæ–­ä¸­...")
        seo = {}
        issues = []
        score = 0
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°
        title = self.soup.find('title')
        if title and title.string:
            title_text = title.string.strip()
            seo['title'] = title_text
            seo['title_length'] = len(title_text)
            if 30 <= len(title_text) <= 60:
                score += 15
            else:
                issues.append(f"ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•ãŒæœ€é©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆç¾åœ¨: {len(title_text)}æ–‡å­—ã€æ¨å¥¨: 30-60æ–‡å­—ï¼‰")
        else:
            issues.append("ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ãŒã‚ã‚Šã¾ã›ã‚“")
            seo['title'] = None
        
        # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³
        meta_desc = self.soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            desc_text = meta_desc.get('content').strip()
            seo['meta_description'] = desc_text
            seo['meta_description_length'] = len(desc_text)
            if 120 <= len(desc_text) <= 160:
                score += 15
            else:
                issues.append(f"ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®é•·ã•ãŒæœ€é©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆç¾åœ¨: {len(desc_text)}æ–‡å­—ã€æ¨å¥¨: 120-160æ–‡å­—ï¼‰")
        else:
            issues.append("ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“")
            seo['meta_description'] = None
        
        # è¦‹å‡ºã—ã‚¿ã‚°ã®åˆ†æ
        headings = {f'h{i}': [] for i in range(1, 7)}
        for i in range(1, 7):
            tags = self.soup.find_all(f'h{i}')
            headings[f'h{i}'] = [tag.get_text().strip() for tag in tags]
        
        seo['headings'] = headings
        
        # H1ã‚¿ã‚°ã®ãƒã‚§ãƒƒã‚¯
        h1_count = len(headings['h1'])
        if h1_count == 1:
            score += 10
        elif h1_count == 0:
            issues.append("H1ã‚¿ã‚°ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            issues.append(f"H1ã‚¿ã‚°ãŒè¤‡æ•°ã‚ã‚Šã¾ã™ï¼ˆ{h1_count}å€‹ï¼‰")
        
        # è¦‹å‡ºã—æ§‹é€ ã®ãƒã‚§ãƒƒã‚¯
        if headings['h2']:
            score += 5
        else:
            issues.append("H2ã‚¿ã‚°ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # ç”»åƒã®altå±æ€§ãƒã‚§ãƒƒã‚¯
        images = self.soup.find_all('img')
        total_images = len(images)
        images_with_alt = sum(1 for img in images if img.get('alt'))
        
        seo['total_images'] = total_images
        seo['images_with_alt'] = images_with_alt
        
        if total_images > 0:
            alt_ratio = images_with_alt / total_images
            if alt_ratio >= 0.9:
                score += 15
            elif alt_ratio >= 0.7:
                score += 10
                issues.append(f"ä¸€éƒ¨ã®ç”»åƒã«altå±æ€§ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ{images_with_alt}/{total_images}ï¼‰")
            else:
                issues.append(f"å¤šãã®ç”»åƒã«altå±æ€§ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ{images_with_alt}/{total_images}ï¼‰")
        
        # ãƒ¡ã‚¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå‚è€ƒæƒ…å ±ï¼‰
        meta_keywords = self.soup.find('meta', attrs={'name': 'keywords'})
        seo['meta_keywords'] = meta_keywords.get('content') if meta_keywords else None
        
        # Open Graphã‚¿ã‚°
        og_tags = {}
        for og in self.soup.find_all('meta', property=re.compile(r'^og:')):
            og_tags[og.get('property')] = og.get('content')
        seo['open_graph'] = og_tags
        if og_tags:
            score += 10
        
        # Twitter Cardã‚¿ã‚°
        twitter_tags = {}
        for twitter in self.soup.find_all('meta', attrs={'name': re.compile(r'^twitter:')}):
            twitter_tags[twitter.get('name')] = twitter.get('content')
        seo['twitter_card'] = twitter_tags
        if twitter_tags:
            score += 5
        
        # Canonicalã‚¿ã‚°
        canonical = self.soup.find('link', rel='canonical')
        seo['canonical'] = canonical.get('href') if canonical else None
        if canonical:
            score += 5
        
        # robotsãƒ¡ã‚¿ã‚¿ã‚°
        robots_meta = self.soup.find('meta', attrs={'name': 'robots'})
        seo['robots_meta'] = robots_meta.get('content') if robots_meta else None
        
        # å†…éƒ¨ãƒªãƒ³ã‚¯æ•°
        internal_links = []
        external_links = []
        for link in self.soup.find_all('a', href=True):
            href = link.get('href')
            if href.startswith('http'):
                if self.domain in href:
                    internal_links.append(href)
                else:
                    external_links.append(href)
            elif href.startswith('/'):
                internal_links.append(href)
        
        seo['internal_links_count'] = len(internal_links)
        seo['external_links_count'] = len(external_links)
        
        if len(internal_links) > 0:
            score += 5
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆJSON-LDï¼‰
        structured_data = []
        for script in self.soup.find_all('script', type='application/ld+json'):
            try:
                data = json.loads(script.string)
                structured_data.append(data)
            except:
                pass
        seo['structured_data'] = structured_data
        if structured_data:
            score += 10
        
        seo['score'] = min(score, 100)
        seo['issues'] = issues
        self.results['seo'] = seo
        print(f"  âœ… SEOã‚¹ã‚³ã‚¢: {seo['score']}/100")
    
    def diagnose_security(self):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨ºæ–­"""
        print("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨ºæ–­ä¸­...")
        security = {}
        issues = []
        score = 0
        
        # HTTPSä½¿ç”¨ç¢ºèª
        is_https = self.parsed_url.scheme == 'https'
        security['https'] = is_https
        if is_https:
            score += 30
        else:
            issues.append("HTTPSã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã›ã‚“ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ï¼‰")
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
        headers = self.response.headers
        security_headers = {
            'Strict-Transport-Security': headers.get('Strict-Transport-Security'),
            'X-Frame-Options': headers.get('X-Frame-Options'),
            'X-Content-Type-Options': headers.get('X-Content-Type-Options'),
            'X-XSS-Protection': headers.get('X-XSS-Protection'),
            'Content-Security-Policy': headers.get('Content-Security-Policy'),
            'Referrer-Policy': headers.get('Referrer-Policy'),
            'Permissions-Policy': headers.get('Permissions-Policy')
        }
        
        security['security_headers'] = security_headers
        
        # å„ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        header_scores = {
            'Strict-Transport-Security': 15,
            'X-Frame-Options': 10,
            'X-Content-Type-Options': 10,
            'X-XSS-Protection': 5,
            'Content-Security-Policy': 20,
            'Referrer-Policy': 5,
            'Permissions-Policy': 5
        }
        
        for header, value in security_headers.items():
            if value:
                score += header_scores.get(header, 0)
            else:
                issues.append(f"{header}ãƒ˜ãƒƒãƒ€ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # SSLè¨¼æ˜æ›¸ã®ç¢ºèªï¼ˆHTTPSã®å ´åˆï¼‰
        if is_https:
            try:
                context = ssl.create_default_context()
                with socket.create_connection((self.domain, 443), timeout=10) as sock:
                    with context.wrap_socket(sock, server_hostname=self.domain) as ssock:
                        cert = ssock.getpeercert()
                        security['ssl_certificate'] = {
                            'issued_to': cert.get('subject', []),
                            'issued_by': cert.get('issuer', []),
                            'valid_from': cert.get('notBefore'),
                            'valid_until': cert.get('notAfter')
                        }
            except Exception as e:
                issues.append(f"SSLè¨¼æ˜æ›¸ã®ç¢ºèªã«å¤±æ•—: {str(e)}")
                security['ssl_certificate'] = None
        
        # Cookieè¨­å®šã®ç¢ºèª
        cookies = self.response.cookies
        security['cookies_count'] = len(cookies)
        secure_cookies = sum(1 for cookie in cookies if cookie.secure)
        httponly_cookies = sum(1 for cookie in cookies if cookie.has_nonstandard_attr('HttpOnly'))
        
        security['secure_cookies'] = secure_cookies
        security['httponly_cookies'] = httponly_cookies
        
        if len(cookies) > 0:
            if secure_cookies == len(cookies):
                score += 5
            if httponly_cookies > 0:
                score += 5
        
        security['score'] = min(score, 100)
        security['issues'] = issues
        self.results['security'] = security
        print(f"  âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {security['score']}/100")
    
    def diagnose_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ºæ–­"""
        print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ºæ–­ä¸­...")
        performance = {}
        issues = []
        score = 0
        
        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿æ™‚é–“
        performance['load_time'] = round(self.load_time, 3)
        if self.load_time < 1:
            score += 30
        elif self.load_time < 2:
            score += 20
            issues.append(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿æ™‚é–“ãŒã‚„ã‚„é…ã„ã§ã™ï¼ˆ{round(self.load_time, 2)}ç§’ï¼‰")
        elif self.load_time < 3:
            score += 10
            issues.append(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿æ™‚é–“ãŒé…ã„ã§ã™ï¼ˆ{round(self.load_time, 2)}ç§’ï¼‰")
        else:
            issues.append(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿æ™‚é–“ãŒéå¸¸ã«é…ã„ã§ã™ï¼ˆ{round(self.load_time, 2)}ç§’ï¼‰")
        
        # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º
        page_size = len(self.response.content)
        performance['page_size_bytes'] = page_size
        performance['page_size_kb'] = round(page_size / 1024, 2)
        
        if page_size < 500 * 1024:  # 500KBæœªæº€
            score += 20
        elif page_size < 1024 * 1024:  # 1MBæœªæº€
            score += 15
            issues.append(f"ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºãŒã‚„ã‚„å¤§ãã„ã§ã™ï¼ˆ{round(page_size/1024, 2)}KBï¼‰")
        elif page_size < 3 * 1024 * 1024:  # 3MBæœªæº€
            score += 5
            issues.append(f"ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºãŒå¤§ãã„ã§ã™ï¼ˆ{round(page_size/1024/1024, 2)}MBï¼‰")
        else:
            issues.append(f"ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºãŒéå¸¸ã«å¤§ãã„ã§ã™ï¼ˆ{round(page_size/1024/1024, 2)}MBï¼‰")
        
        # ãƒªã‚½ãƒ¼ã‚¹æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ
        resources = {
            'scripts': len(self.soup.find_all('script')),
            'stylesheets': len(self.soup.find_all('link', rel='stylesheet')),
            'images': len(self.soup.find_all('img')),
            'iframes': len(self.soup.find_all('iframe'))
        }
        performance['resources'] = resources
        
        total_resources = sum(resources.values())
        if total_resources < 30:
            score += 15
        elif total_resources < 50:
            score += 10
        else:
            issues.append(f"ãƒªã‚½ãƒ¼ã‚¹æ•°ãŒå¤šã™ãã¾ã™ï¼ˆåˆè¨ˆ: {total_resources}ï¼‰")
        
        # åœ§ç¸®ã®ç¢ºèª
        content_encoding = self.response.headers.get('Content-Encoding')
        performance['compression'] = content_encoding
        if content_encoding in ['gzip', 'br', 'deflate']:
            score += 15
        else:
            issues.append("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åœ§ç¸®ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¶å¾¡
        cache_control = self.response.headers.get('Cache-Control')
        performance['cache_control'] = cache_control
        if cache_control:
            score += 10
        else:
            issues.append("Cache-Controlãƒ˜ãƒƒãƒ€ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ç”»åƒã®æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯
        large_images = []
        for img in self.soup.find_all('img'):
            src = img.get('src')
            if src and not src.startswith('data:'):
                # ç”»åƒã‚µã‚¤ã‚ºã®ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯çœç•¥ï¼‰
                if src.endswith(('.jpg', '.jpeg', '.png', '.gif')):
                    # å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€è­¦å‘Šã®ã¿
                    pass
        
        # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ã®ä½¿ç”¨
        inline_styles = len(self.soup.find_all(style=True))
        performance['inline_styles_count'] = inline_styles
        if inline_styles > 10:
            issues.append(f"ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ãŒå¤šç”¨ã•ã‚Œã¦ã„ã¾ã™ï¼ˆ{inline_styles}å€‹ï¼‰")
        
        performance['score'] = min(score, 100)
        performance['issues'] = issues
        self.results['performance'] = performance
        print(f"  âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {performance['score']}/100")
    
    def diagnose_accessibility(self):
        """ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£è¨ºæ–­"""
        print("â™¿ ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£è¨ºæ–­ä¸­...")
        accessibility = {}
        issues = []
        score = 0
        
        # è¨€èªå±æ€§
        html_tag = self.soup.find('html')
        lang = html_tag.get('lang') if html_tag else None
        accessibility['lang_attribute'] = lang
        if lang:
            score += 15
        else:
            issues.append("HTMLè¦ç´ ã«langå±æ€§ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # ç”»åƒã®altå±æ€§ï¼ˆå†ç¢ºèªï¼‰
        images = self.soup.find_all('img')
        images_without_alt = [img for img in images if not img.get('alt')]
        accessibility['images_without_alt_count'] = len(images_without_alt)
        
        if len(images) > 0:
            alt_ratio = (len(images) - len(images_without_alt)) / len(images)
            if alt_ratio == 1:
                score += 20
            elif alt_ratio >= 0.8:
                score += 15
                issues.append(f"{len(images_without_alt)}å€‹ã®ç”»åƒã«altå±æ€§ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                issues.append(f"å¤šãã®ç”»åƒã«altå±æ€§ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ{len(images_without_alt)}/{len(images)}ï¼‰")
        
        # ãƒ•ã‚©ãƒ¼ãƒ ãƒ©ãƒ™ãƒ«
        inputs = self.soup.find_all(['input', 'textarea', 'select'])
        inputs_with_labels = 0
        for inp in inputs:
            inp_id = inp.get('id')
            aria_label = inp.get('aria-label')
            if inp_id and self.soup.find('label', attrs={'for': inp_id}):
                inputs_with_labels += 1
            elif aria_label:
                inputs_with_labels += 1
        
        accessibility['form_inputs_count'] = len(inputs)
        accessibility['inputs_with_labels'] = inputs_with_labels
        
        if len(inputs) > 0:
            if inputs_with_labels == len(inputs):
                score += 15
            elif inputs_with_labels >= len(inputs) * 0.7:
                score += 10
                issues.append(f"ä¸€éƒ¨ã®ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã«ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ{inputs_with_labels}/{len(inputs)}ï¼‰")
            else:
                issues.append(f"å¤šãã®ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã«ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ{inputs_with_labels}/{len(inputs)}ï¼‰")
        
        # ARIAå±æ€§ã®ä½¿ç”¨
        aria_elements = self.soup.find_all(attrs={'role': True})
        aria_labels = self.soup.find_all(attrs={'aria-label': True})
        accessibility['aria_roles_count'] = len(aria_elements)
        accessibility['aria_labels_count'] = len(aria_labels)
        
        if len(aria_elements) > 0 or len(aria_labels) > 0:
            score += 10
        
        # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯è¦ç´ 
        landmarks = ['header', 'nav', 'main', 'footer', 'aside', 'section', 'article']
        found_landmarks = {tag: len(self.soup.find_all(tag)) for tag in landmarks}
        accessibility['landmarks'] = found_landmarks
        
        if found_landmarks['main'] > 0:
            score += 10
        else:
            issues.append("mainãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
        
        if found_landmarks['nav'] > 0:
            score += 5
        
        # è¦‹å‡ºã—ã®éšå±¤æ§‹é€ 
        headings_order = []
        for i in range(1, 7):
            for tag in self.soup.find_all(f'h{i}'):
                headings_order.append(i)
        
        # è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ã®é£›ã°ã—ã‚’ãƒã‚§ãƒƒã‚¯
        heading_skip = False
        for i in range(len(headings_order) - 1):
            if headings_order[i+1] - headings_order[i] > 1:
                heading_skip = True
                break
        
        if not heading_skip and len(headings_order) > 0:
            score += 10
        elif heading_skip:
            issues.append("è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ãŒé£›ã°ã•ã‚Œã¦ã„ã¾ã™ï¼ˆh2ã®å¾Œã«h4ãªã©ï¼‰")
        
        # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚§ãƒƒã‚¯
        links = self.soup.find_all('a')
        empty_links = [link for link in links if not link.get_text().strip() and not link.get('aria-label')]
        accessibility['empty_links_count'] = len(empty_links)
        
        if len(empty_links) == 0 and len(links) > 0:
            score += 10
        elif len(empty_links) > 0:
            issues.append(f"{len(empty_links)}å€‹ã®ãƒªãƒ³ã‚¯ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
        
        # ã‚¿ãƒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¢ºèª
        negative_tabindex = self.soup.find_all(attrs={'tabindex': lambda x: x and int(x) < 0})
        if len(negative_tabindex) > 0:
            issues.append(f"è² ã®tabindexå€¤ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ï¼ˆ{len(negative_tabindex)}å€‹ï¼‰")
        
        accessibility['score'] = min(score, 100)
        accessibility['issues'] = issues
        self.results['accessibility'] = accessibility
        print(f"  âœ… ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {accessibility['score']}/100")
    
    def calculate_overall_score(self):
        """ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        weights = {
            'seo': 0.3,
            'security': 0.3,
            'performance': 0.2,
            'accessibility': 0.2
        }
        
        overall = 0
        for category, weight in weights.items():
            overall += self.results[category]['score'] * weight
        
        self.results['overall_score'] = round(overall, 1)
        print(f"\nğŸ¯ ç·åˆã‚¹ã‚³ã‚¢: {self.results['overall_score']}/100")
    
    def generate_report(self):
        """è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = []
        report.append("=" * 80)
        report.append(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 80)
        report.append(f"URL: {self.url}")
        report.append(f"è¨ºæ–­æ—¥æ™‚: {self.results['timestamp']}")
        report.append(f"ç·åˆã‚¹ã‚³ã‚¢: {self.results['overall_score']}/100")
        report.append("=" * 80)
        report.append("")
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°
        categories = [
            ('SEOè¨ºæ–­', 'seo', 'ğŸ“Š'),
            ('ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨ºæ–­', 'security', 'ğŸ”’'),
            ('ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ºæ–­', 'performance', 'âš¡'),
            ('ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£è¨ºæ–­', 'accessibility', 'â™¿')
        ]
        
        for title, key, icon in categories:
            data = self.results[key]
            report.append(f"\n{icon} {title}")
            report.append("-" * 80)
            report.append(f"ã‚¹ã‚³ã‚¢: {data['score']}/100")
            
            if data['issues']:
                report.append("\nã€æ”¹å–„ãŒå¿…è¦ãªé …ç›®ã€‘")
                for issue in data['issues']:
                    report.append(f"  â€¢ {issue}")
            else:
                report.append("\nâœ… å•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            report.append("")
        
        return "\n".join(report)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 80)
    print("ğŸ” ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆè¨ºæ–­ãƒ„ãƒ¼ãƒ«")
    print("=" * 80)
    print("")
    
    # URLã®å…¥åŠ›
    url = input("è¨ºæ–­ã™ã‚‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    
    if not url:
        print("âŒ URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    print("")
    
    # è¨ºæ–­ã®å®Ÿè¡Œ
    tool = WebsiteDiagnosisTool(url)
    results = tool.run_diagnosis()
    
    if results:
        print("\n" + "=" * 80)
        print(tool.generate_report())
        
        # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = f"diagnosis_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è©³ç´°ãªè¨ºæ–­çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
